
# Path to pretrained UniLM model files
model:
  model_recover_path: "./unilm-base-model.bin"    # Path to the pretrained checkpoint
  vocab_path: "./vocab.txt"                      # Local tokenizer vocab
  config_path: "./config.json"                   # Model architecture config
  bert_model: "bert-base-uncased"                # Can be kept as a name or directory

# Input/Output data
data:
  data_dir: "./data"    # Root directory for all data files
  train_src: "./data/cnn_dm/train.src" ###Source input for training (used in run_seq2seq.py)
  train_tgt: "./data/cnn_dm/train.tgt" ###Target output for training (used in run_seq2seq.py)
  dev_src: "./data/cnn_dm/dev.src" ###Source input for validation (optional, used in run_seq2seq.py)
  dev_tgt: "./data/cnn_dm/dev.tgt" ###Target output for validation (optional, used in run_seq2seq.py)
  input_file: "./data/cnn_dm/test.src" ###Source input for decoding (used in decode_seq2seq.py)
  output_file: "./results/test.out" #### Output file for model predictions (written by decode_seq2seq.py)

# Tokenization
tokenizer:
  do_lower_case: true
  max_seq_length: 128

# Training hyperparameters
training:
  do_train: true
  do_eval: true
  finetune_decay: true
  output_dir: "./finetuned_unilm"
  num_train_epochs: 3
  per_device_train_batch_size: 32
  eval_batch_size: 64
  learning_rate: 5e-5
  label_smoothing: 0.0
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  weight_decay: 0.01
  warmup_proportion: 0.1
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  optim_recover_path: "./finetuned_unilm/optim.latest.bin"
  fp16: true
  fp32_embedding: false
  loss_scale: 0.0
  amp: true
  from_scratch: false
  new_segment_ids: false
  new_pos_ids: false
  tokenized_input: false
  max_len_a: 0
  max_len_b: 0
  trunc_seg: ""
  always_truncate_tail: false
  mask_prob: 0.15
  mask_prob_eos: 0.0
  max_pred: 20
  num_workers: 0
  mask_source_words: false
  skipgram_prb: 0.0
  skipgram_size: 1
  mask_whole_word: false
  do_l2r_training: false
  has_sentence_oracle: false
  max_position_embeddings: null
  relax_projection: false
  ffn_type: 0
  num_qkv: 0
  seg_emb: false
  s2s_special_token: false
  s2s_add_segment: false
  s2s_share_segment: false
  pos_shift: false

decode:
  subset: 0
  batch_size: 4
  length_penalty: 0.0
  max_tgt_length: 128

# Logging and saving
logging:
  log_dir: "./logs"
  logging_steps: 50
  save_steps: 500
  save_total_limit: 2
  seed: 42

# Generation/inference (for decode_seq2seq.py)
generation:
  beam_size: 5
  length_penalty: 0.6
  forbid_duplicate_ngrams: true
  ngram_size: 3
  min_len: 5
